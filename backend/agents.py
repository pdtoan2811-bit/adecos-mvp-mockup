"""
crewAI Agent Definitions for AI Agent Feature

Defines specialized agents for:
- Intent routing
- Data querying and analysis
- Narrative generation
"""

import os
import json
import logging
from typing import Optional
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from data_tools import get_all_tools, QueryAdsCampaignsTool, CalculateMetricsTool
from intent_classifier import classify_intent
import os
import json
import logging
from typing import Optional
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from data_tools import get_all_tools, QueryAdsCampaignsTool, CalculateMetricsTool
from intent_classifier import classify_intent
from google import genai
from dotenv import load_dotenv

load_dotenv()

# Configure logging for debug visibility
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("AI_AGENT")
logger.setLevel(logging.DEBUG)

# Create console handler with formatting
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    formatter = logging.Formatter('[%(name)s] %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)

# Configure Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
client = genai.Client(api_key=GOOGLE_API_KEY)


class GeminiLLM:
    """Simple wrapper to use Gemini as the LLM for crewAI agents."""
    
    def __init__(self, model_name: str = "gemini-3-flash-preview"):
        self.model_name = model_name
        self.client = client
    
    def __call__(self, prompt: str) -> str:
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt
        )
        return response.text


# Initialize Gemini LLM
gemini_llm = GeminiLLM()


def create_router_agent() -> Agent:
    """Create the Router Agent that classifies user intent."""
    return Agent(
        role="Intent Router",
        goal="Accurately classify user queries into intent categories and route to appropriate handlers",
        backstory="""You are an expert at understanding user intent in the context of 
        affiliate marketing and ads management. You analyze queries to determine if users 
        want data analysis, explanations, or follow-up on previous responses.""",
        verbose=True,
        allow_delegation=False,
        llm=gemini_llm
    )


def create_data_analyst_agent() -> Agent:
    """Create the Data Analyst Agent that queries and analyzes data."""
    return Agent(
        role="Data Analyst",
        goal="Query relevant data and provide insightful analysis of ads performance",
        backstory="""You are a skilled data analyst specializing in digital advertising metrics.
        You can query campaign data, calculate KPIs, identify trends, and spot anomalies.
        You always provide data in formats suitable for visualization.""",
        tools=get_all_tools(),
        verbose=True,
        allow_delegation=False,
        llm=gemini_llm
    )


def create_narrative_agent() -> Agent:
    """Create the Narrative Agent that generates empathetic, contextual responses."""
    return Agent(
        role="Narrative Writer",
        goal="Generate empathetic, contextual narratives that introduce data insights",
        backstory="""You are an expert communicator who transforms data insights into 
        compelling narratives. You write in Vietnamese, using a friendly yet professional tone.
        You always provide context before showing data and highlight key takeaways.""",
        verbose=True,
        allow_delegation=False,
        llm=gemini_llm
    )


# Local classify_intent function removed in favor of imported version from intent_classifier.py



async def execute_data_analysis_crew(query: str, entities: dict) -> dict:
    """Execute the data analysis crew for data visualization requests."""
    
    logger.info(f"üìä EXECUTING DATA ANALYSIS for: '{query}'")
    logger.debug(f"   Entities: {entities}")
    
    # Step 1: Query the data
    query_tool = QueryAdsCampaignsTool()
    calc_tool = CalculateMetricsTool()
    
    time_range = entities.get("time_range") or "last 30 days"
    breakdown = entities.get("breakdown")
    visual_type = entities.get("visual_type") # Explicit user request: line, bar, etc.
    
    logger.info(f"üìÖ Time range: {time_range} | Breakdown: {breakdown} | Visual: {visual_type}")
    
    # Get campaign data
    query_params = {
        "date_range": time_range,
        "group_by": entities.get("group_by", "day")
    }
    
    # If granular breakdown requested (e.g. "compare accounts over time")
    if breakdown in ["account", "campaign"] and "theo" in query.lower() and ("ng√†y" in query.lower() or "th√°ng" in query.lower() or "over time" in query.lower() or "bi·ªÉu ƒë·ªì" in query.lower()):
        query_params["breakdown"] = breakdown
    
    # Add optional filters
    if entities.get("program"):
        query_params["program"] = entities["program"]
    if entities.get("keywords"):
        query_params["keywords"] = entities["keywords"]
    
    data_result = query_tool._run(json.dumps(query_params))
    data_parsed = json.loads(data_result)
    
    is_granular = data_parsed.get("is_granular", False)
    logger.debug(f"   Data points retrieved: {len(data_parsed['data'])} | Granular: {is_granular}")
    
    # Calculate metrics
    metrics_result = calc_tool._run(json.dumps({
        "data": data_parsed["data"],
        "metrics": ["cpc", "roas", "ctr"]
    }))
    metrics_parsed = json.loads(metrics_result)
    
    # Step 2: Generate narrative
    narrative_prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch qu·∫£ng c√°o.
Ng∆∞·ªùi d√πng ƒëang h·ªèi: "{query}"

D·ªØ li·ªáu t·ªïng h·ª£p ({time_range}):
- Clicks: {data_parsed['summary']['totalClicks']:,}
- Cost: {data_parsed['summary']['totalCost']:,.0f}
- Revenue: {data_parsed['summary']['totalRevenue']:,.0f}
- CPC: {metrics_parsed['metrics'].get('cpc', 0):,.0f}
- ROAS: {metrics_parsed['metrics'].get('roas', 0):.2f}
- CTR: {metrics_parsed['metrics'].get('ctr', 0):.2f}%

Y√™u c·∫ßu logic:
1. ƒê·ªçc k·ªπ c√¢u h·ªèi ng∆∞·ªùi d√πng ƒë·ªÉ bi·∫øt h·ªç quan t√¢m ch·ªâ s·ªë n√†o.
2. Vi·∫øt nh·∫≠n ƒë·ªãnh t·∫≠p trung v√†o c√¢u h·ªèi ƒë√≥. 
3. N·∫øu l√† so s√°nh (breakdown), h√£y nh·∫≠n x√©t xu h∆∞·ªõng c·ªßa c√°c entities.
4. Ng·∫Øn g·ªçn (2-3 c√¢u). Ti·∫øng Vi·ªát.
"""

    narrative_response = await client.aio.models.generate_content(
        model="gemini-3-flash-preview",
        contents=narrative_prompt
    )
    narrative = narrative_response.text.strip()
    
    # Step 3: Prepare Visualization Data
    chart_data = data_parsed["data"]
    series = []
    chart_title = "Hi·ªáu su·∫•t qu·∫£ng c√°o"
    chart_type = visual_type if visual_type in ["line", "bar", "area"] else "area" # Use user pref or default
    x_axis_key = "date"
    
    # COLOR PALETTE for multi-series
    colors = ["#3b82f6", "#ef4444", "#22c55e", "#f59e0b", "#8b5cf6", "#06b6d4", "#ec4899", "#6366f1"]
    
    # Logic for Multi-Series Chart (Pivoting)
    if is_granular:
        # Pivot logic: Transform [{date, entity, cost}, ...] -> [{date, entity1_cost, entity2_cost}, ...]
        pivoted = {}
        entities_found = set()
        
        # Determine metric to plot
        metric_key = "cost" # Default
        if "doanh thu" in query.lower() or "revenue" in query.lower(): metric_key = "revenue"
        elif "click" in query.lower(): metric_key = "clicks"
        elif "cpc" in query.lower(): metric_key = "cpc"
        elif "roas" in query.lower(): metric_key = "roas"
        
        chart_title = f"{metric_key.upper()} theo {breakdown} ({time_range})"
        if not visual_type: chart_type = "line" # Default to line for comparison over time
        
        for record in data_parsed["data"]:
            d_key = record["date"]
            ent = record["entity"]
            entities_found.add(ent)
            
            if d_key not in pivoted:
                pivoted[d_key] = {"date": d_key}
            
            pivoted[d_key][ent] = record[metric_key]
            
        chart_data = sorted(list(pivoted.values()), key=lambda x: x["date"])
        
        # Generate series for each entity
        for idx, ent in enumerate(sorted(entities_found)):
            color = colors[idx % len(colors)]
            series.append({
                "dataKey": ent,
                "name": ent,
                "color": color
            })
            
    else:
        # Standard Single Series Logic (Bar/Line/Area for total metrics)
        query_lower = query.lower()
        
        # Check for specific metrics mentioned
        if "cpc" in query_lower:
            if not visual_type: chart_type = "line"
            if not series: series.append({"dataKey": "cpc", "name": "CPC", "color": "#3b82f6"})
        if "roas" in query_lower:
             if not visual_type: chart_type = "line"
             if not series: series.append({"dataKey": "roas", "name": "ROAS", "color": "#8b5cf6"})
        if "ctr" in query_lower:
            if not visual_type: chart_type = "line"
            if not series: series.append({"dataKey": "ctr", "name": "CTR %", "color": "#06b6d4"})
        if "click" in query_lower:
            if not visual_type: chart_type = "line"
            if not series: series.append({"dataKey": "clicks", "name": "Clicks", "color": "#3b82f6"})
        if "impression" in query_lower:
             if not visual_type: chart_type = "area"
             if not series: series.append({"dataKey": "impressions", "name": "Impressions", "color": "#8b5cf6"})
        if "conversions" in query_lower:
             chart_type = "bar"
             if not series: series.append({"dataKey": "conversions", "name": "Chuy·ªÉn ƒë·ªïi", "color": "#f59e0b"})
        
        # Fallback if no specific metric found
        if not series:
             series = [
                {"dataKey": "cost", "name": "Chi ph√≠", "color": "#ef4444"},
                {"dataKey": "revenue", "name": "Doanh thu", "color": "#22c55e"}
            ]
             if not visual_type: chart_type = "area"

    logger.info(f"üìà CHART: {chart_type} | SERIES: {len(series)} | DATA: {len(chart_data)}")
    
    return {
        "type": "composite",
        "content": {
            "sections": [
                {
                    "type": "narrative",
                    "content": narrative
                },
                {
                    "type": "chart",
                    "content": {
                        "chartType": chart_type,
                        "title": f"{chart_title}",
                        "data": chart_data,
                        "config": {
                            "xAxis": x_axis_key,
                            "series": series
                        }
                    }
                }
            ],
            "summary": metrics_parsed
        },
        "context": {
            "filters": {
                "timeRange": time_range,
                "dateRange": data_parsed.get("dateRange"), # Pass structured start/end dates
                "program": entities.get("program"),
                "keywords": entities.get("keywords")
            },
            "followupSuggestions": [
                "So s√°nh v·ªõi th√°ng tr∆∞·ªõc",
                "Ph√¢n t√≠ch theo chi·∫øn d·ªãch", 
                "Chi ti·∫øt h∆°n v·ªÅ d·ªØ li·ªáu n√†y"
            ]
        }
    }



async def execute_explanation_crew(query: str, conversation_history: str = "") -> dict:
    """Execute explanation response for conceptual questions."""
    
    prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia affiliate marketing th√¢n thi·ªán.

Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch d·ªÖ hi·ªÉu:

C√¢u h·ªèi: {query}

Ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥: {conversation_history if conversation_history else "Ch∆∞a c√≥"}

Y√™u c·∫ßu:
- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu
- D√πng v√≠ d·ª• th·ª±c t·∫ø khi c·∫ßn
- Format v·ªõi markdown khi ph√π h·ª£p
- Th√¢n thi·ªán nh∆∞ng chuy√™n nghi·ªáp"""

    response = await client.aio.models.generate_content(
        model="gemini-3-flash-preview",
        contents=prompt
    )
    
    return {
        "type": "text",
        "content": response.text.strip()
    }


async def execute_data_query_crew(query: str, entities: dict) -> dict:
    """Execute data query for table/list requests."""
    
    from data_tools import QueryCampaignListTool, QueryAccountsTool
    
    # Determine what data to query
    query_lower = query.lower()
    
    if "campaign" in query_lower or "chi·∫øn d·ªãch" in query_lower:
        tool = QueryCampaignListTool()
        
        # Build query with filters
        params = {}
        if entities.get("program"):
            params["program"] = entities["program"]
        if entities.get("keywords"):
             kws = entities["keywords"]
             if isinstance(kws, list) and kws:
                 params["keyword"] = kws[0]
             elif isinstance(kws, str):
                 params["keyword"] = kws
        
        result = tool._run(json.dumps(params))
        data = json.loads(result)
        table_data = data["campaigns"]
        
        filter_desc = ""
        if params.get("program"):
            filter_desc += f" cho {params['program']}"
        if params.get("keyword"):
            filter_desc += f" v·ªõi t·ª´ kh√≥a '{params['keyword']}'"
            
        narrative = f"D∆∞·ªõi ƒë√¢y l√† danh s√°ch {len(table_data)} chi·∫øn d·ªãch{filter_desc}:"
    elif "account" in query_lower or "t√†i kho·∫£n" in query_lower:
        tool = QueryAccountsTool()
        result = tool._run("")
        data = json.loads(result)
        table_data = data["accounts"]
        narrative = f"B·∫°n ƒëang c√≥ {data['activeAccounts']} t√†i kho·∫£n ƒëang ho·∫°t ƒë·ªông trong t·ªïng s·ªë {data['totalAccounts']} t√†i kho·∫£n:"
    else:
        # Default to campaigns with filters if any
        tool = QueryCampaignListTool()
        
        params = {}
        if entities.get("program"):
            params["program"] = entities["program"]
        if entities.get("keywords"):
             kws = entities["keywords"]
             if isinstance(kws, list) and kws:
                 params["keyword"] = kws[0]
             elif isinstance(kws, str):
                 params["keyword"] = kws
                 
        result = tool._run(json.dumps(params))
        data = json.loads(result)
        table_data = data["campaigns"]
        narrative = f"ƒê√¢y l√† d·ªØ li·ªáu b·∫°n y√™u c·∫ßu:"
    
    return {
        "type": "composite",
        "content": {
            "sections": [
                {
                    "type": "narrative",
                    "content": narrative
                },
                {
                    "type": "table",
                    "content": table_data
                }
            ]
        }
    }


async def execute_research_crew(query: str, entities: dict, conversation_history: str = "") -> dict:
    """Execute affiliate program research - returns table of program recommendations.
    
    This reuses the old research functionality to find affiliate programs in a niche.
    """
    
    niche = entities.get("niche", query)  # Use query as niche if not extracted
    
    # Research prompt template (same as old generator.py)
    prompt = f"""Research Niche: {niche}
Context from previous conversation (if any):
{conversation_history if conversation_history else ""}

Generate 5-10 high-quality affiliate programs (native or network) relevant to this niche in Vietnam (or global programs popular in Vietnam).
If the niche is vague (e.g. "more", "others"), use the Context to determine the actual topic.

For each program, provide:
- brand: Name of the brand.
- program_url: Direct link to affiliate page.
- commission_percent: Commission percentage as number (e.g., 10 for 10%, 15 for 15%). If CPA/flat rate, use 0.
- commission_type: Type of commission ("percentage", "cpa", "hybrid").
- can_use_brand: Boolean (true/false) - whether affiliates can use brand name in ads.
- traffic_3m: Estimated monthly visits or trend (e.g., "500k/th√°ng", "12M+").
- legitimacy_score: A confidence score (0-10) based on brand reputation.

Return ONLY the JSON array.
"""
    
    response = await client.aio.models.generate_content(
        model="gemini-3-flash-preview",
        contents=prompt
    )
    
    # Parse the response
    buffer = response.text.strip()

    
    # Post-process: Strip markdown wrappers if present
    if buffer.startswith('```'):
        lines = buffer.split('\n')
        if lines[0].startswith('```'):
            lines = lines[1:]
        if lines and lines[-1].strip() == '```':
            lines = lines[:-1]
        buffer = '\n'.join(lines).strip()
    
    try:
        parsed = json.loads(buffer)
        if isinstance(parsed, dict) and 'content' in parsed:
            table_data = parsed['content']
        elif isinstance(parsed, list):
            table_data = parsed
        else:
            table_data = []
    except json.JSONDecodeError:
        table_data = [{"error": "Kh√¥ng th·ªÉ parse k·∫øt qu·∫£ t·ª´ AI"}]
    
    # Generate a brief narrative introduction
    narrative = f"ƒê√¢y l√† c√°c ch∆∞∆°ng tr√¨nh affiliate trong lƒ©nh v·ª±c **{niche}** m√† t√¥i t√¨m ƒë∆∞·ª£c cho b·∫°n:"
    
    return {
        "type": "composite",
        "content": {
            "sections": [
                {
                    "type": "narrative",
                    "content": narrative
                },
                {
                    "type": "table",
                    "content": table_data
                }
            ]
        },
        "context": {
            "niche": niche,
            "followupSuggestions": [
                f"Th√™m programs trong lƒ©nh v·ª±c {niche}",
                "So s√°nh commission rates",
                "Ng√°ch li√™n quan kh√°c"
            ]
        }
    }


async def run_agent_workflow(messages: list) -> dict:
    """Main entry point for the agent workflow.
    
    Args:
        messages: List of message dicts with 'role' and 'content'
    
    Returns:
        Response dict with type and content
    """
    
    logger.info("=" * 60)
    logger.info("ü§ñ AI AGENT WORKFLOW STARTED")
    logger.info("=" * 60)
    
    # Get the latest user message
    user_messages = [m for m in messages if m.get('role') == 'user']
    if not user_messages:
        return {"type": "text", "content": "Kh√¥ng t√¨m th·∫•y tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng."}
    
    query = user_messages[-1].get('content', '')
    logger.info(f"üìù USER QUERY: '{query}'")
    logger.debug(f"   Total messages in context: {len(messages)}")
    
    # Build conversation history for context
    conversation_history = ""
    for msg in messages[:-1]:
        role = msg.get('role', 'user')
        content = msg.get('content', '')
        if isinstance(content, str):
            conversation_history += f"{role}: {content}\n"
        elif isinstance(content, dict):
            # Summarize previous response
            conversation_history += f"{role}: [Previous data/chart response]\n"
    
    # Step 1: Classify intent
    # Note: classify_intent from intent_classifier is async
    intent_result = await classify_intent(query, conversation_history)
    intent = intent_result.get("intent", "data_analysis")
    entities = intent_result.get("entities", {})
    
    logger.info(f"üéØ ROUTING TO: {intent.upper()}")
    
    # Step 2: Route to appropriate crew
    if intent == "data_analysis" or intent == "comparison":
        return await execute_data_analysis_crew(query, entities)
    elif intent == "data_query":
        return await execute_data_query_crew(query, entities)
    elif intent == "explanation":
        return await execute_explanation_crew(query, conversation_history)
    elif intent == "research":
        return await execute_research_crew(query, entities, conversation_history)
    elif intent == "followup":
        # For followup, try to understand what type of followup
        if any(word in query.lower() for word in ["t·∫°i sao", "why", "gi·∫£i th√≠ch", "explain"]):
            return await execute_explanation_crew(query, conversation_history)
        else:
            return await execute_data_analysis_crew(query, entities)
    
    # Default fallback
    return await execute_explanation_crew(query, conversation_history)

